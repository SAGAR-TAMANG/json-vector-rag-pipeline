{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = \"FAQ.json\"\n",
    "\n",
    "# Load the JSON File as Documents\n",
    "loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize an Open-Source Embedding Model (using SentenceTransformers)\n",
    "model_name = \"all-MiniLM-L6-v2\"  # Lightweight and free\n",
    "# Create a HuggingFaceEmbeddings instance which internally uses SentenceTransformers\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Create a FAISS Vector Store from the Documents\n",
    "faiss_db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# (Optional) Persist the FAISS index locally for later use.\n",
    "faiss_db.save_local(\"data/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query text\n",
    "query = \"What delivery options?\"\n",
    "\n",
    "# Perform a similarity search\n",
    "results = faiss_db.similarity_search(query, )\n",
    "\n",
    "# Print out the results\n",
    "for result in results:\n",
    "    print(result)  # or result.metadata, depending on what you stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the Computer is restarted then you can load local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load the persisted FAISS index with dangerous deserialization allowed\n",
    "faiss_db = FAISS.load_local(\"data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"how to create something?\"\n",
    "results = faiss_db.similarity_search(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to add other json files (i.e., new data) into this existing vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths for the new JSON files\n",
    "new_files = [\n",
    "    \"privacy-policy.json\",\n",
    "    \"common-questions.json\"\n",
    "]\n",
    "\n",
    "# Initialize your embedding model (ensure it's the same as used before)\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load the existing FAISS index from disk, allowing dangerous deserialization if needed\n",
    "faiss_db = FAISS.load_local(\"data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Loop over each new JSON file, load documents and add them to the index\n",
    "for file_path in new_files:\n",
    "    # Load the JSON file as documents\n",
    "    loader = JSONLoader(file_path=file_path, jq_schema=\".[]\", text_content=False)\n",
    "    new_documents = loader.load()\n",
    "    \n",
    "    # Add the new documents to the existing FAISS index\n",
    "    faiss_db.add_documents(new_documents)\n",
    "\n",
    "# Optionally, persist the updated FAISS index\n",
    "faiss_db.save_local(\"data/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Load the persisted FAISS index with dangerous deserialization allowed\n",
    "faiss_db = FAISS.load_local(\"data/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"troubleshoot\"\n",
    "results = faiss_db.similarity_search(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def query_faiss(user_query, index_path=\"data/faiss_index\", top_k=5):\n",
    "    \"\"\"\n",
    "    Given a user query, this function loads the persisted FAISS index,\n",
    "    performs a similarity search, and returns the top_k matching documents.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The input query from the user.\n",
    "        index_path (str): The path where the FAISS index is saved.\n",
    "        top_k (int): Number of similar documents to return.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of the top_k matching document objects.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding model (should match the one used to build the index)\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    \n",
    "    # Load the persisted FAISS index (allow dangerous deserialization if needed)\n",
    "    faiss_db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # Perform similarity search with the given query\n",
    "    results = faiss_db.similarity_search(user_query, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your query: \")\n",
    "    similar_docs = query_faiss(query)\n",
    "    \n",
    "    print(\"\\nTop matching documents:\")\n",
    "    for doc in similar_docs:\n",
    "        print(\"-----\")\n",
    "        print(doc.page_content)  # or use doc.metadata if that's more appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamlitCallbackHandler  # example callback for streaming output\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables (make sure OPENAI_API_KEY is set)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def construct_rag_prompt(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Build a prompt that includes retrieved document content and the user's query.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant that uses the following context to answer the question.\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"Note: If the 'Question' the user asks is not present in the 'Context', then you will respond with 'Unfortunately we do not have answer for that.\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Get user's query\n",
    "user_query = input(\"Enter your query: \")\n",
    "\n",
    "# Step 1: Retrieve similar documents using the FAISS index\n",
    "retrieved_docs = query_faiss(user_query, index_path=\"data/faiss_index\", top_k=3)\n",
    "\n",
    "# Step 2: Construct the RAG prompt\n",
    "prompt = construct_rag_prompt(user_query, retrieved_docs)\n",
    "print(\"\\n\\n--- RAG Prompt ---\\n\")\n",
    "print(prompt)\n",
    "print(\"\\n\\n--- Streaming Response ---\\n\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions using provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    stream=True,  # Enable streaming mode\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "answer = \"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
